
@ilyasut (Ilya Sutskever):
One point I made that didn’t come across:
- Scaling the current thing will keep leading to improvements.  In particular, it won’t stall.
- But something important will continue to be missing.
>  QT @slow_developer:
> here are the most important points from today's ilya sutskever podcast:
> 
> - superintelligence in 5-20 years
> - current scaling will stall hard; we're back to real research
>  https://x.com/slow_developer/status/1993416904162328880
date: Fri Nov 28 15:13:51 +0000 2025
url: https://x.com/ilyasut/status/1994424504370581726
──────────────────────────────────────────────────

@ilyasut (Ilya Sutskever):
Important work
>  QT @AnthropicAI:
> New Anthropic research: Natural emergent misalignment from reward hacking in production RL.
> 
> “Reward hacking” is where models learn to cheat on tasks they’re given during training.
> 
> VIDEO: https://pbs.twimg.com/media/G6TWZSGWEAAnZfv.jpg
>  https://x.com/AnthropicAI/status/1991952400899559889
date: Sat Nov 22 20:24:37 +0000 2025
url: https://x.com/ilyasut/status/1992328386258317591
──────────────────────────────────────────────────
